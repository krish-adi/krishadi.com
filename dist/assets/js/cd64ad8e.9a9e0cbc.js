"use strict";(self.webpackChunkkrishadi_com=self.webpackChunkkrishadi_com||[]).push([[468],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>h});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=p(a),u=r,h=m["".concat(s,".").concat(u)]||m[u]||c[u]||o;return a?n.createElement(h,i(i({ref:t},d),{},{components:a})):n.createElement(h,i({ref:t},d))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[m]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},165:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const o={slug:"/machine-learning",title:"Machine Learning",description:"Scikit-learn Python quick reference cheat sheet",published:"4/18/2022",last_update:{date:"2/21/2022"}},i="Machine Learning",l={unversionedId:"kast/machine-learning",id:"kast/machine-learning",title:"Machine Learning",description:"Scikit-learn Python quick reference cheat sheet",source:"@site/zettelkasten/kast/03-machine-learning.md",sourceDirName:"kast",slug:"/machine-learning",permalink:"/zettelkasten/machine-learning",draft:!1,tags:[],version:"current",lastUpdatedAt:1645398e3,formattedLastUpdatedAt:"Feb 20, 2022",sidebarPosition:3,frontMatter:{slug:"/machine-learning",title:"Machine Learning",description:"Scikit-learn Python quick reference cheat sheet",published:"4/18/2022",last_update:{date:"2/21/2022"}},sidebar:"zettelkastenSidebar",previous:{title:"Rheology",permalink:"/zettelkasten/rheology"},next:{title:"Pandas",permalink:"/zettelkasten/pandas"}},s={},p=[{value:"Data exploration",id:"data-exploration",level:2},{value:"Building the model",id:"building-the-model",level:2},{value:"Decision tree",id:"decision-tree",level:2},{value:"Model quality",id:"model-quality",level:2},{value:"Validating the model",id:"validating-the-model",level:2},{value:"Different models",id:"different-models",level:2},{value:"Random forests",id:"random-forests",level:2},{value:"Cleaning data part 2",id:"cleaning-data-part-2",level:2},{value:"Pipelines",id:"pipelines",level:2},{value:"Step 1: Define Preprocessing Steps",id:"step-1-define-preprocessing-steps",level:3},{value:"Step 2: Define the Model",id:"step-2-define-the-model",level:3},{value:"Step 3: Create and Evaluate the Pipeline",id:"step-3-create-and-evaluate-the-pipeline",level:3},{value:"Cross validation",id:"cross-validation",level:2},{value:"Data leakage",id:"data-leakage",level:2},{value:"Gradient Boosting",id:"gradient-boosting",level:2},{value:"<code>n_estimators</code>",id:"n_estimators",level:3},{value:"<code>early_stopping_rounds</code>",id:"early_stopping_rounds",level:3},{value:"<code>learning_rate</code>",id:"learning_rate",level:3},{value:"<code>n_jobs</code>",id:"n_jobs",level:3}],d={toc:p},m="wrapper";function c(e){let{components:t,...o}=e;return(0,r.kt)(m,(0,n.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"machine-learning"},"Machine Learning"),(0,r.kt)("h2",{id:"data-exploration"},"Data exploration"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# save filepath to variable for easier access\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\n\n# read the data and store data in DataFrame titled melbourne_data\nmelbourne_data = pd.read_csv(melbourne_file_path)\n\n# print a summary of the data in Melbourne data\nmelbourne_data.describe()\n")),(0,r.kt)("h1",{id:"clean-data"},"Clean data"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# The Melbourne data has some missing values (some houses for which some variables weren't recorded.)\n# We'll learn to handle missing values in a later tutorial.\n# Your Iowa data doesn't have missing values in the columns you use.\n# So we will take the simplest option for now, and drop houses from our data.\n# Don't worry about this much for now, though the code is:\n\n# dropna drops missing values (think of na as \"not available\")\nmelbourne_data = melbourne_data.dropna(axis=0)\n")),(0,r.kt)("h1",{id:"selecting-prediction-target"},"Selecting prediction target"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"y = melbourne_data.Price\n")),(0,r.kt)("h1",{id:"choosing-features"},"Choosing features"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n\nX = melbourne_data[melbourne_features]\n")),(0,r.kt)("h2",{id:"building-the-model"},"Building the model"),(0,r.kt)("p",null,"The steps to building and using a model are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Define:"),"\xa0What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Fit:"),"\xa0Capture patterns from provided data. This is the heart of modeling."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Predict:"),"\xa0Just what it sounds like"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Evaluate"),": Determine how accurate the model's predictions are.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.tree import DecisionTreeRegressor\n\n# Define model. Specify a number for random_state to ensure same results each run\nmelbourne_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model\nmelbourne_model.fit(X, y)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'print("Making predictions for the following 5 houses:")\nprint(X.head())\n\nprint("The predictions are")\nprint(melbourne_model.predict(X.head()))\n')),(0,r.kt)("h2",{id:"decision-tree"},"Decision tree"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Decision tree",src:a(7647).Z,width:"709",height:"449"})),(0,r.kt)("h2",{id:"model-quality"},"Model quality"),(0,r.kt)("p",null,"There are many metrics for summarizing model quality, but we'll start with one called\xa0",(0,r.kt)("strong",{parentName:"p"},"Mean Absolute Error"),"\xa0(also called\xa0",(0,r.kt)("strong",{parentName:"p"},"MAE"),")."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"error = actual \u2212 predicted\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n")),(0,r.kt)("h2",{id:"validating-the-model"},"Validating the model"),(0,r.kt)("p",null,"Since models' practical value come from making predictions on new data, we measure performance on data that wasn't used to build the model. The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. This data is called\xa0",(0,r.kt)("strong",{parentName:"p"},"validation data"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))\n")),(0,r.kt)("h2",{id:"different-models"},"Different models"),(0,r.kt)("p",null,"There are different models that can be explored. Scikit-learn's\xa0",(0,r.kt)("a",{parentName:"p",href:"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"},"documentation"),"\xa0that the decision tree model has many options."),(0,r.kt)("p",null,"A phenomenon called\xa0",(0,r.kt)("strong",{parentName:"p"},"overfitting"),", where a model matches the training data almost perfectly, but does poorly in validation and other new data. When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called\xa0",(0,r.kt)("strong",{parentName:"p"},"underfitting"),"."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Under fit and over fit",src:a(3582).Z,width:"1814",height:"1268"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Function to return the MAE for different parameters of the decision tree model.\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print("Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))\n')),(0,r.kt)("p",null,"Max leaf nodes: 5 Mean Absolute Error: 347380\nMax leaf nodes: 50 Mean Absolute Error: 258171\nMax leaf nodes: 500 Mean Absolute Error: 243495\nMax leaf nodes: 5000 Mean Absolute Error: 254983"),(0,r.kt)("h2",{id:"random-forests"},"Random forests"),(0,r.kt)("p",null,"The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest#sklearn.ensemble.RandomForestRegressor"},"Scikit-Learn-Random-Forest-Regressor")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))\n")),(0,r.kt)("h2",{id:"cleaning-data-part-2"},"Cleaning data part 2"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Drop columns and/or rows")," with missing values"),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Imputation")," fills in the missing values with some number."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Extended imputation")," we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries. Adding ",(0,r.kt)("inlineCode",{parentName:"li"},"True")," or ",(0,r.kt)("inlineCode",{parentName:"li"},"False ")," to the imputed data by row or column.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.impute import SimpleImputer\n\n# Imputation\n# SimpleIMputer to replace it by the mean value along the column.\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Make copy to avoid changing original data (when imputing)\nX_train_plus = X_train.copy()\nX_valid_plus = X_valid.copy()\n\n# Make new columns indicating what will be imputed\nfor col in cols_with_missing:\n    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\nimputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n\n# Imputation removed column names; put them back\nimputed_X_train_plus.columns = X_train_plus.columns\nimputed_X_valid_plus.columns = X_valid_plus.columns\n\nprint(\"MAE from Approach 3 (An Extension to Imputation):\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Categorical variables")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Drop Categorical Variables")," remove them from the dataset. This approach will only work well if the columns did not contain useful information."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Ordinal Encoding")," box them into categories and label them with numbers. Works well with tree based models decision and random forest."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"One-Hot Encoding")," suppose an color is given, enocde each of them into three column values of each R G B. One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# String object types are considered to be of categorical.\n\n# Get list of categorical variables\n\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"drop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\nprint(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from sklearn.preprocessing import OrdinalEncoder\n\n# Make copy to avoid changing original data\nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\n# Apply ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nlabel_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n\nprint("MAE from Approach 2 (Ordinal Encoding):")\nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n')),(0,r.kt)("p",null,"the\xa0",(0,r.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"},(0,r.kt)("inlineCode",{parentName:"a"},"OneHotEncoder")),"\xa0class from scikit-learn to get one-hot encodings. There are a number of parameters that can be used to customize its behavior."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"We set\xa0",(0,r.kt)("inlineCode",{parentName:"li"},"handle_unknown='ignore'"),"\xa0to avoid errors when the validation data contains classes that aren't represented in the training data, and"),(0,r.kt)("li",{parentName:"ul"},"setting\xa0",(0,r.kt)("inlineCode",{parentName:"li"},"sparse=False"),"\xa0ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).")),(0,r.kt)("p",null,"To use the encoder, we supply only the categorical columns that we want to be one-hot encoded. For instance, to encode the training data, we supply\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"X_train[object_cols]"),". (",(0,r.kt)("inlineCode",{parentName:"p"},"object_cols"),"\xa0in the code cell below is a list of the column names with categorical data, and so\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"X_train[object_cols]"),"\xa0contains all of the categorical data in the training set.)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1) # axis = 1 since they share same row numbers.\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\nprint(\"MAE from Approach 3 (One-Hot Encoding):\")\nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n")),(0,r.kt)("p",null,"Usage of ",(0,r.kt)("inlineCode",{parentName:"p"},"set")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Categorical columns in the training data\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if\n                   set(X_valid[col]).issubset(set(X_train[col]))]\n\n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n")),(0,r.kt)("h2",{id:"pipelines"},"Pipelines"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Pipelines"),"\xa0are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step."),(0,r.kt)("p",null,"Many data scientists hack together models without pipelines, but pipelines have some important benefits. Those include:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Cleaner Code:"),"\xa0Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Fewer Bugs:"),"\xa0There are fewer opportunities to misapply a step or forget a preprocessing step."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Easier to Productionize:"),"\xa0It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"More Options for Model Validation:"),"\xa0You will see an example in the next tutorial, which covers cross-validation.")),(0,r.kt)("p",null,"We construct the full pipeline in three steps."),(0,r.kt)("h3",{id:"step-1-define-preprocessing-steps"},"Step 1: Define Preprocessing Steps"),(0,r.kt)("p",null,"Similar to how a pipeline bundles together preprocessing and modeling steps, we use the\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"ColumnTransformer"),"\xa0class to bundle together different preprocessing steps. The code below:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"imputes missing values in\xa0",(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"numerical")),"\xa0data, and"),(0,r.kt)("li",{parentName:"ul"},"imputes missing values and applies a one-hot encoding to\xa0",(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"categorical")),"\xa0data.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n")),(0,r.kt)("h3",{id:"step-2-define-the-model"},"Step 2: Define the Model"),(0,r.kt)("p",null,"Next, we define a random forest model with the familiar\xa0",(0,r.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"},(0,r.kt)("inlineCode",{parentName:"a"},"RandomForestRegressor")),"\xa0class."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n")),(0,r.kt)("h3",{id:"step-3-create-and-evaluate-the-pipeline"},"Step 3: Create and Evaluate the Pipeline",(0,r.kt)("a",{parentName:"h3",href:"https://www.kaggle.com/code/alexisbcook/pipelines#Step-3:-Create-and-Evaluate-the-Pipeline"})),(0,r.kt)("p",null,"Finally, we use the\xa0",(0,r.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"},(0,r.kt)("inlineCode",{parentName:"a"},"Pipeline")),"\xa0class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"With the pipeline, we preprocess the training data and fit the model in a single line of code. (",(0,r.kt)("em",{parentName:"li"},"In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!"),")"),(0,r.kt)("li",{parentName:"ul"},"With the pipeline, we supply the unprocessed features in\xa0",(0,r.kt)("inlineCode",{parentName:"li"},"X_valid"),"\xa0to the\xa0",(0,r.kt)("inlineCode",{parentName:"li"},"predict()"),"\xa0command, and the pipeline automatically preprocesses the features before generating predictions. (",(0,r.kt)("em",{parentName:"li"},"However, without a pipeline, we have to remember to preprocess the validation data before making predictions."),")")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"\nfrom sklearn.metrics import mean_absolute_error\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model\nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)\n")),(0,r.kt)("h2",{id:"cross-validation"},"Cross validation"),(0,r.kt)("p",null,"In\xa0",(0,r.kt)("strong",{parentName:"p"},"cross-validation"),", we run our modeling process on different subsets of the data to get multiple measures of model quality."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"For small datasets"),", where extra computational burden isn't a big deal, you should run cross-validation."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"For larger datasets"),", a single validation set is sufficient. Your code will run faster, and you may have enough data that there's little need to re-use some of it for holdout.")),(0,r.kt)("p",null,"There's no simple threshold for what constitutes a large vs. small dataset. But if your model takes a couple minutes or less to run, it's probably worth switching to cross-validation."),(0,r.kt)("p",null,"Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment yields the same results, a single validation set is probably sufficient."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nmy_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n                               ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n                               ])\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\\n\", scores)\n")),(0,r.kt)("h2",{id:"data-leakage"},(0,r.kt)("a",{parentName:"h2",href:"https://www.kaggle.com/code/alexisbcook/data-leakage#Introduction"},"Data leakage")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Data leakage"),"\xa0(or\xa0",(0,r.kt)("strong",{parentName:"p"},"leakage"),") happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Target leakage")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Target leakage"),"\xa0occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the\xa0",(0,r.kt)("em",{parentName:"p"},"timing or chronological order"),"\xa0that data becomes available, not merely whether a feature helps make good predictions."),(0,r.kt)("p",null,"How to detect this? If a data feature changes or is influenced by an event after the value of the target is noted. People testing positive for covid and a column with taking specific medications for covid."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Train-Test Contamination")),(0,r.kt)("p",null,"Validation is meant to be a measure of how the model does on data that it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavior. This is sometimes called\xa0",(0,r.kt)("strong",{parentName:"p"},"train-test contamination"),". If your validation is based on a simple train-test split, exclude the validation data from any type of\xa0",(0,r.kt)("em",{parentName:"p"},"fitting"),", including the fitting of preprocessing steps."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!)\nmy_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\ncv_scores = cross_val_score(my_pipeline, X, y,\n                            cv=5,\n                            scoring='accuracy')\n\nprint(\"Cross-validation accuracy: %f\" % cv_scores.mean())\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"expenditures_cardholders = X.expenditure[y]\nexpenditures_noncardholders = X.expenditure[~y]\n\nprint('Fraction of those who did not receive a card and had no expenditures: %.2f' \\\n      %((expenditures_noncardholders == 0).mean()))\nprint('Fraction of those who received a card and had no expenditures: %.2f' \\\n      %(( expenditures_cardholders == 0).mean()))\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Drop leaky predictors from dataset\npotential_leaks = ['expenditure', 'share', 'active', 'majorcards']\nX2 = X.drop(potential_leaks, axis=1)\n\n# Evaluate the model with leaky predictors removed\ncv_scores = cross_val_score(my_pipeline, X2, y,\n                            cv=5,\n                            scoring='accuracy')\n\nprint(\"Cross-val accuracy: %f\" % cv_scores.mean())\n")),(0,r.kt)("h1",{id:"xgboost"},"XGBoost"),(0,r.kt)("h2",{id:"gradient-boosting"},(0,r.kt)("a",{parentName:"h2",href:"https://www.kaggle.com/code/alexisbcook/xgboost#Gradient-Boosting"},"Gradient Boosting")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Gradient boosting"),"\xa0is a method that goes through cycles to iteratively add models into an ensemble."),(0,r.kt)("p",null,"It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.)"),(0,r.kt)("p",null,"Then, we start the cycle:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble."),(0,r.kt)("li",{parentName:"ul"},"These predictions are used to calculate a loss function (like\xa0",(0,r.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Mean_squared_error"},"mean squared error"),", for instance)."),(0,r.kt)("li",{parentName:"ul"},"Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. (",(0,r.kt)("em",{parentName:"li"},'Side note: The "gradient" in "gradient boosting" refers to the fact that we\'ll use\xa0',(0,r.kt)("a",{parentName:"em",href:"https://en.wikipedia.org/wiki/Gradient_descent"},"gradient descent"),"\xa0on the loss function to determine the parameters in this new model."),")"),(0,r.kt)("li",{parentName:"ul"},"Finally, we add the new model to ensemble, and ..."),(0,r.kt)("li",{parentName:"ul"},"... repeat!")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://i.imgur.com/MvCGENh.png",alt:"tut6_boosting"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\nmy_model.fit(X_train, y_train)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from sklearn.metrics import mean_absolute_error\n\npredictions = my_model.predict(X_valid)\nprint("Mean Absolute Error: " + str(mean_absolute_error(predictions, y_valid)))\n')),(0,r.kt)("h3",{id:"n_estimators"},(0,r.kt)("a",{parentName:"h3",href:"https://www.kaggle.com/code/alexisbcook/xgboost#n_estimators"},(0,r.kt)("inlineCode",{parentName:"a"},"n_estimators"))),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"n_estimators"),"\xa0specifies how many times to go through the modeling cycle, and equal to the number of models that we include in the ensemble. Too\xa0",(0,r.kt)("em",{parentName:"p"},"low"),"\xa0a value causes\xa0",(0,r.kt)("em",{parentName:"p"},"underfitting"),", Too\xa0",(0,r.kt)("em",{parentName:"p"},"high"),"\xa0a value causes\xa0",(0,r.kt)("em",{parentName:"p"},"overfitting"),". Typical values range from 100-1000, though this depends a lot on the\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"learning_rate"),"\xa0parameter discussed below."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train)\n")),(0,r.kt)("h3",{id:"early_stopping_rounds"},(0,r.kt)("a",{parentName:"h3",href:"https://www.kaggle.com/code/alexisbcook/xgboost#early_stopping_rounds"},(0,r.kt)("inlineCode",{parentName:"a"},"early_stopping_rounds"))),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"early_stopping_rounds"),"\xa0offers a way to automatically find the ideal value for\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"n_estimators"),", causes the model to stop iterating when the validation score stops improving. Set a high value for\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"n_estimators"),"\xa0and then use\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"early_stopping_rounds"),". Setting\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"early_stopping_rounds=5"),"\xa0is a reasonable choice, to stop after 5 straight rounds of deteriorating validation scores. Using\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"early_stopping_rounds"),", you set aside some data for calculating the validation scores - by setting the\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"eval_set"),"\xa0parameter."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train,\n             early_stopping_rounds=5,\n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\n")),(0,r.kt)("h3",{id:"learning_rate"},(0,r.kt)("a",{parentName:"h3",href:"https://www.kaggle.com/code/alexisbcook/xgboost#learning_rate"},(0,r.kt)("inlineCode",{parentName:"a"},"learning_rate"))),(0,r.kt)("p",null,"We can multiply the predictions from each model by a small number (known as the\xa0",(0,r.kt)("strong",{parentName:"p"},"learning rate"),") before adding them in. We can set a higher value for\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"n_estimators"),"\xa0without overfitting. A small learning rate and large number of estimators will yield more accurate XGBoost models."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train, y_train,\n             early_stopping_rounds=5,\n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\n")),(0,r.kt)("h3",{id:"n_jobs"},(0,r.kt)("a",{parentName:"h3",href:"https://www.kaggle.com/code/alexisbcook/xgboost#n_jobs"},(0,r.kt)("inlineCode",{parentName:"a"},"n_jobs"))),(0,r.kt)("p",null,"You can use parallelism to build your models faster. Set the parameter\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"n_jobs"),"\xa0equal to the number of cores on your machine. Useful in large datasets where you would otherwise spend a long time waiting during the\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"fit"),"\xa0command. On smaller datasets, this won't help."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(X_train, y_train,\n             early_stopping_rounds=5,\n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\n")),(0,r.kt)("hr",null),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"References")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Scikit documentation"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.kaggle.com/"},"Kaggle"))))}c.isMDXComponent=!0},7647:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/decision-tree-2fca3eb5f2df365c008264139f7cc174.png"},3582:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/under-over-fit-cf9d09e78dc613bb56a27996baba9774.png"}}]);